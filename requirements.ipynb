{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Requirements\n",
    "\n",
    "# \n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "As part of this exercise we will be performing two operations on a large dataset using Numpy. First, considering an array of values, we will compute the percentage differential from one value to the next, as in the example below.\n",
    "``` example\n",
    "    input_array = [43, 63, 72, 64, 16]\n",
    "    percent_diff = [(43-63)/43, (63-72)/63, (72-64)/72, (64-16)/64]\n",
    "```\n",
    "Second, we will compute the cummulative sum on an input array:\n",
    "``` example\n",
    "    input_array = [43, 63, 72, 64, 16]\n",
    "    cumsum = [43, 43+63, 43+63+72, 43+63+72+64, 43+63+72+64+16]\n",
    "```\n",
    "Working code for both functions is provided in the following cells, the task is to re-write both functions to accelerate the computation. Several means of speeding up code have been covered . Those include the use of caching, the use of native languages, the use of parallel computing, or the modification of the default algorithm. You can use one or several combined strategies.\n",
    "\n",
    "---\n",
    "\n",
    "The following cell's code is from [https://github.com/rakshitha123/TSForecasting/blob/master/utils/data_loader.py] and enables the convert an input dataset in .tsf format into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit, prange\n",
    "\n",
    "# https://github.com/rakshitha123/TSForecasting/blob/master/utils/data_loader.py\n",
    "\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "\n",
    "# Converts the contents in a .tsf file into a dataframe and returns it along\n",
    "# with other meta-data of the dataset: frequency, horizon, whether the dataset\n",
    "# contains missing values and whether the series have equal lengths\n",
    "#\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series\n",
    "# in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the\n",
    "# column containing series values in the returning dataframe\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\") as file:  # , encoding=\"cp1252\"\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if len(line_content) != 3:  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if len(line_content) != 2:  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(strtobool(line_content[1]))\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\"Missing attribute section. \"\n",
    "                                            \"Attribute section must come before data.\")\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\"Missing attribute section. \"\n",
    "                                        \"Attribute section must come before data.\")\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated \"\n",
    "                                \"numeric values. At least one numeric value should be there \"\n",
    "                                \"in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(numeric_series):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains \"\n",
    "                                \"a set of comma separated numeric values. At least one numeric \"\n",
    "                                \"value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(full_info[i], \"%Y-%m-%d %H-%M-%S\")\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )   # Currently, the code supports only numeric, string and date types.\n",
    "                                    # Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "\n",
    "# Example of usage\n",
    "# loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\"TSForecasting/tsf_data/sample.tsf\")\n",
    "\n",
    "# print(loaded_data)\n",
    "# print(frequency)\n",
    "# print(forecast_horizon)\n",
    "# print(contain_missing_values)\n",
    "# print(contain_equal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads a dataset downloaded from [https://zenodo.org/record/7371038].\n",
    "The following description is from the dataset webpage:\n",
    "\n",
    "\"This dataset contains 145063 time series representing the number of hits or web traffic for a set of Wikipedia pages from 2015-07-01 to 2022-06-30. This is an extended version of the dataset that was used in the Kaggle Wikipedia Web Traffic forecasting competition. For consistency, the same Wikipedia pages that were used in the competition have been used in this dataset as well. The colons (:) in article names have been replaced by dashes (-) to make the .tsf file readable using our data loaders.\n",
    "\n",
    "The original dataset contains missing values. They have been simply replaced by zeros.\n",
    "\n",
    "The data were downloaded from the Wikimedia REST API. According to the conditions of the API, this dataset is licensed under CC-BY-SA 3.0 and GFDL licenses.\"\n",
    "\n",
    "You can download the dataset from [https://zenodo.org/record/7371038/files/web_traffic_extended_dataset_without_missing_values.zip]. Note that the file is large: 433 MB. The cells convert the dataset into a Numpy compressed file for easier use in the remainder of this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenodo.org/record/7371038\n",
    "df, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(\n",
    "    \"web_traffic_extended_dataset_without_missing_values.tsf\"\n",
    ")\n",
    "hits_arr = np.stack(df[\"series_value\"].apply(np.asarray).tolist())\n",
    "np.savez_compressed(\"web_traffic_extended_dataset_without_missing_values\", hits_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset and extract a single Serie\n",
    "hits_arr = np.load(\"web_traffic_extended_dataset_without_missing_values.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implement the `pct_change1` function, used to compute the percentage difference change beetween succesive values, as aforementioned. The cell is also used to time the function and display the result as a figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change1(n):\n",
    "    result = np.diff(n) / n[:, :-1]\n",
    "    result[np.isnan(result)] = 0  # difference is 0 and both values are 0\n",
    "    result[np.isinf(result)] = 1  # first value was 0 and next is not 0\n",
    "    return result\n",
    "\n",
    "\n",
    "pct = pct_change1(hits_arr)\n",
    "\n",
    "%timeit pct_change1(hits_arr)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(pct[10559, 300:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1  \n",
    "\n",
    "Modify the following cell to implement the function `pct_change2` to accelerate the computation. The `np.allclose()` function checks that the results obtained using `pct_change1` and `pct_change2` are near identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change2(n):\n",
    "    result = None\n",
    "    #TODO\n",
    "    return result\n",
    "\n",
    "\n",
    "pct_n = pct_change2(hits_arr)\n",
    "\n",
    "%timeit pct_change2(hits_arr)\n",
    "\n",
    "print(np.allclose(pct, pct_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cumsum` function from numpy can be used to compute the cummulative sum of the `hits_arr` array, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs1 = np.cumsum(hits_arr, 1)\n",
    "\n",
    "%timeit np.cumsum(hits_arr,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2  \n",
    "\n",
    "Modify the following cell to implement the function `cumsum2` to accelerate the computation. The `np.allclose()` function enables to check that the results obtained using `np.cumsum` and `cumsum2` are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum2(n):\n",
    "    result = None\n",
    "    #TODO\n",
    "    return result\n",
    "\n",
    "\n",
    "cs2 = cumsum2(hits_arr)\n",
    "\n",
    "%timeit cumsum2(hits_arr)\n",
    "\n",
    "np.allclose(cs1, cs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "The aim of the exercise is to develop a effective solution to solve the [travelling salesman problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem).\n",
    "\n",
    "The idea of the Travelling Salesman Problem is to find the shortest path necessary to visit all locations and return to the start. For an example of visiting four locations, cycle length is the distance travelled from starting location A to location B, then to location C, then to location D, then back to A. The brute force approach to solving this is consider every possible cycle between the locations, however for N locations there are N! possible cycles so this isn't really tractable for anything above small values like 10.\n",
    "\n",
    "The following cell generate a random graph with 100 nodes, links the nodes in indices increasing order (to create edges between pair of nodes) and display the resulting solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import permutations\n",
    "from timeit import timeit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "def create_locations(num, width):\n",
    "    return np.random.uniform(0, width, (num, 2))\n",
    "\n",
    "def plot_cycle(locs, cycle):\n",
    "    x, y = zip(*locs)\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y)\n",
    "\n",
    "    for idx in range(cycle.shape[0]):\n",
    "        x0, y0 = locs[cycle[idx]]\n",
    "        x1, y1 = locs[cycle[(idx + 1) % locs.shape[0]]]\n",
    "        plt.plot([x0, x1], [y0, y1], c=\"r\",linewidth=1)\n",
    "\n",
    "locs = create_locations(100, 10)\n",
    "cycle = np.arange(len(locs))\n",
    "plot_cycle(locs, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimise the traveling salesman problem, one need to compute the overall length of the current solution. The following cell provide a numpy based implementation in the `cycle_length_numpy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_length_numpy(locs,cycle):\n",
    "    locs_ordered=locs[cycle]\n",
    "    return np.linalg.norm(locs_ordered - np.roll(locs_ordered, 1, 0), axis=1).sum()\n",
    "\n",
    "\n",
    "print(cycle_length_numpy(locs,cycle))\n",
    "%timeit cycle_length_numpy(locs,cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 \n",
    "\n",
    "Using the techniques from the course implement an optimised version of the algorithm, assuming your input array of locations has shape `(N, 2)` (ie. `N` 2D coordinates) and array of cycle indices has shape `(N,)`. It's possible to achieve of speedup of at least 10 times over the algorithm given above without using threading/multiprocessing so do not use these techniques.\n",
    "\n",
    "Using Numpy requires the creation of a number of extra arrays used for calculation, however an optimised one can be implemented which doesn't need to create any additional arrays but instead accumulates a result value. You should consider this in your implementation and you will be marked on whether you can avoid this extra allocation.\n",
    "\n",
    "You are expected to provide a solution to the `cycle_length` function in the following cell. The `cycle_length` function should be optimised in term of computation time when compare to the numpy `cycle_length_numpy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_length_fast(locs, cycle):\n",
    "    \"\"\"Compute the length of cycle given by the indices `cycle` and positions `locs`.\"\"\"\n",
    "    length = None\n",
    "    # TODO\n",
    "    return length\n",
    "\n",
    "\n",
    "print(cycle_length_fast(locs, cycle))\n",
    "%timeit cycle_length_fast(locs,cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 \n",
    "\n",
    "Reimplement your algorithm from above to use threading and see how it's performance is relative to the fast and Numpy versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_length_thread(locs, cycle):\n",
    "    \"\"\"Compute the length of cycle given by the indices `cycle` and positions `locs`.\"\"\"\n",
    "    length = None\n",
    "    # TODO\n",
    "    return length\n",
    "\n",
    "\n",
    "print(cycle_length_thread(locs, cycle))\n",
    "%timeit cycle_length_thread(locs, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "\n",
    "Times for runs of the three versions of the algorithm are calculated below and plotted. At certain points one version of the algorithm or another becomes the fastest, why is this? Specifically at what point does the threaded version become fastest and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_sizes = [10, 100, 1_000, 10_000, 100_000, 1_000_000]\n",
    "locs = create_locations(loc_sizes[-1], 10)\n",
    "cycle = np.arange(loc_sizes[-1])\n",
    "\n",
    "numpy_times = [timeit(lambda: cycle_length_numpy(locs, cycle[:n]), number=20) for n in loc_sizes]\n",
    "fast_times = [timeit(lambda: cycle_length_fast(locs, cycle[:n]), number=20) for n in loc_sizes]\n",
    "thread_times = [timeit(lambda: cycle_length_thread(locs, cycle[:n]), number=20) for n in loc_sizes]\n",
    "\n",
    "plt.loglog(loc_sizes, numpy_times, label=\"Numpy\")\n",
    "plt.loglog(loc_sizes, fast_times, label=\"Fast\")\n",
    "plt.loglog(loc_sizes, thread_times, label=\"Thread\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the previous question in this cell [200 word answer max].\n",
    "\n",
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding a Good Cycle\n",
    "\n",
    "A number of approaches exist to find a good cycle for non-trivial location sizes. There are exact algorithms that are faster than brute force but are still NP-hard. A approximate solution involves some technique to find a good solution in some defined manner that's better than just chance. The implementation below uses a genetic algorithm approach to start from an initial cycle (parent) and modify parts of it in a randomised way a set number of times. From the generated cycles (offspring) the shortest is chosen to be the parent of the next generation. This has similarities to genetic mutations in real organisms but simplifies the concepts immensely but allows one to start from one solution and attempt to find a nearby better solution, ultimately with the hope of converging onto a final solution that's close to an actual optimum. This approach is relatively simple and attractive for attacking real-world problems with intractable algorithmic solutions. See: https://en.wikipedia.org/wiki/Genetic_algorithm\n",
    "\n",
    "Below is the brute force solution to the problem, don't run this with locations counts above 10 because it will take enormous amounts of time to finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cycle_bf(locs):\n",
    "    \"\"\"Try all possible cycles for the given locations, this will take longer than the age of the universe for even trivial sets.\"\"\"\n",
    "    nlocs = len(locs)\n",
    "    indices = np.arange(nlocs)\n",
    "    min_len = float(\"inf\")\n",
    "    result = None\n",
    "\n",
    "    for p in permutations(np.arange(nlocs), nlocs):\n",
    "        indices[:] = p\n",
    "        # modify the cycle length to use your fastest implementation\n",
    "        clen = cycle_length_numpy(locs, indices)\n",
    "        if clen < min_len:\n",
    "            min_len = clen\n",
    "            result = p\n",
    "\n",
    "    return result\n",
    "\n",
    "test_locs = create_locations(10, 10)\n",
    "best = find_best_cycle_bf(test_locs)\n",
    "print(cycle_length_numpy(test_locs, np.array(best)))\n",
    "plot_cycle(test_locs, np.array(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 \n",
    "\n",
    "Below is the solution using a genetic algorithm approach. For a set number of iterations, a starting cycle is mutated a randomised number of times to produce a number of offspring. From these the shortest path is chosen as the new parent and all others go extinct. \n",
    "\n",
    "The following function is used to mutate a given cycle. Optimise this using the techniques discussed in the course. Consider what it's doing as the form of mutation it applies and implement some additional operation that gives a better result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mutation(cycle, num_changes):\n",
    "    \"\"\"Given a cycle, apply `num_changes` mutations to the cycle and return the mutant.\"\"\"\n",
    "    cycle = cycle.copy()\n",
    "\n",
    "    for _ in range(np.random.randint(1, num_changes)):\n",
    "        i,j = np.random.randint(0, len(cycle),(2,))\n",
    "        cycle[i], cycle[j] = cycle[j], cycle[i]\n",
    "\n",
    "    return cycle\n",
    "\n",
    "generate_mutation(cycle,10)\n",
    "\n",
    "%timeit generate_mutation(cycle,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mutation_fast(cycle, num_changes):\n",
    "    \"\"\"Given a cycle, apply `num_changes` mutations to the cycle and return the mutant.\"\"\"\n",
    "    cycle = None\n",
    "    # TODO\n",
    "    return cycle\n",
    "\n",
    "generate_mutation_fast(cycle,10)\n",
    "\n",
    "%timeit generate_mutation_fast(cycle,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_better_cycle(locs, init_cycle, num_iterations, num_offspring, num_changes):\n",
    "    current_cycle = init_cycle\n",
    "    iterlengths = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # modify the generate_mutation to use your fastest implementation\n",
    "        offspring = [generate_mutation(current_cycle, num_changes) for _ in range(num_offspring)]\n",
    "        family = [current_cycle] + offspring\n",
    "\n",
    "        # modify the cycle length to use your fastest implementation\n",
    "        lengths = np.array([cycle_length_numpy(locs, c) for c in family])\n",
    "\n",
    "        idx = min(range(len(lengths)), key=lengths.__getitem__)\n",
    "\n",
    "        current_cycle = family[idx]\n",
    "        iterlengths.append(lengths[idx])\n",
    "\n",
    "    return current_cycle, iterlengths\n",
    "\n",
    "new_cycle=None\n",
    "new_lengths=None\n",
    "\n",
    "def _timeit_func():\n",
    "    global new_cycle\n",
    "    global new_lengths\n",
    "    \n",
    "    new_cycle, new_lengths = find_better_cycle(\n",
    "        locs=locs,\n",
    "        init_cycle=cycle,\n",
    "        num_iterations=20000,\n",
    "        num_offspring=100,\n",
    "        num_changes=40\n",
    "    )\n",
    "    \n",
    "    \n",
    "tval=timeit(_timeit_func,number=1,globals=globals())\n",
    "\n",
    "print(f\"Time: {tval}, Best length: {new_lengths[-1]}\")\n",
    "\n",
    "plt.semilogy(new_lengths)\n",
    "plot_cycle(locs, new_cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 \n",
    "\n",
    "Modify the cell below (copy/pasted from cell above) to improve the result (shortest path) of the retrieved solution and/or improve the overall computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_better_cycle(locs, init_cycle, num_iterations, num_offspring, num_changes):\n",
    "    current_cycle = init_cycle\n",
    "    iterlengths = []\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # modify the generate_mutation to use your fastest implementation\n",
    "        offspring = [generate_mutation(current_cycle, num_changes) for _ in range(num_offspring)]\n",
    "        family = [current_cycle] + offspring\n",
    "\n",
    "        # modify the cycle length to use your fastest implementation\n",
    "        lengths = np.array([cycle_length_numpy(locs, c) for c in family])\n",
    "\n",
    "        idx = min(range(len(lengths)), key=lengths.__getitem__)\n",
    "\n",
    "        current_cycle = family[idx]\n",
    "        iterlengths.append(lengths[idx])\n",
    "\n",
    "    return current_cycle, iterlengths\n",
    "\n",
    "new_cycle=None\n",
    "new_lengths=None\n",
    "\n",
    "def _timeit_func():\n",
    "    global new_cycle\n",
    "    global new_lengths\n",
    "    \n",
    "    new_cycle, new_lengths = find_better_cycle(\n",
    "        locs=locs,\n",
    "        init_cycle=cycle,\n",
    "        num_iterations=20000,\n",
    "        num_offspring=100,\n",
    "        num_changes=40\n",
    "    )\n",
    "    \n",
    "    \n",
    "tval=timeit(_timeit_func,number=1,globals=globals())\n",
    "\n",
    "print(f\"Time: {tval}, Best length: {new_lengths[-1]}\")\n",
    "\n",
    "plt.semilogy(new_lengths)\n",
    "plot_cycle(locs, new_cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall:\n",
    "While writing your code and comments, you are expected to adhere to Python\n",
    "style guideline (see [1]). Your code will be assessed with static and style\n",
    "analysis tools  to detect any potential\n",
    "defects.\n",
    "\n",
    "Your code is expected to use features of the python language\n",
    "For example, you should use iterator, generator or list\n",
    "comprehension [2]  instead of `for` loop when appropriate.\n",
    "\n",
    "You are expected to comment your code. Ensure that your\n",
    "comments are meaningful. You can refer to [3] for recommendations about\n",
    "good commenting practices.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Reference:\n",
    "\n",
    "[1] https://www.python.org/dev/peps/pep-0008\n",
    "\n",
    "[2] https://www.w3schools.com/python/python_lists_comprehension.asp\n",
    "\n",
    "[3] https://realpython.com/python-comments-guide/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
